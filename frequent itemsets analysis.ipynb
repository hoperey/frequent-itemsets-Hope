{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5561fc32",
   "metadata": {},
   "source": [
    "Name: Hope Kimandi\n",
    "ADM no: 670317"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb9994cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved supermarket_transactions.csv\n"
     ]
    }
   ],
   "source": [
    "# Student: Hope\n",
    "\n",
    "import csv                     # used to save the generated transactions into a CSV file\n",
    "import random                  # used to randomly select items for each transaction\n",
    "import pandas as pd            # imported for creating tables\n",
    "import itertools               # imported for generating combinations\n",
    "import math                    # imported for math operations \n",
    "\n",
    "random.seed(42)                # fixing the random seed so results stay the same every run\n",
    "\n",
    "num_transactions = 4000        # total number of supermarket transactions to simulate\n",
    "min_items = 2                  # minimum number of items per transaction\n",
    "max_items = 7                  # maximum number of items per transaction\n",
    "\n",
    "# creating a pool of 33 unique items that customers can buy\n",
    "item_pool = [\n",
    "    'apples','grapes','bananas','juice','oranges',\n",
    "    'tomatoes','kiwi','cookies','milk','potatoes',\n",
    "    'yogurt','rice','onion','cereal','flour','cakes',\n",
    "    'bread','butter','peanut butter','sugar','salt',\n",
    "    'coffee','tea','chocolate','eggs','soap','pens','pencil',\n",
    "    'shampoo','toothpaste','detergent','napkins','olive oil'\n",
    "]\n",
    "\n",
    "transactions = []              # empty list where all generated transactions will be stored\n",
    "\n",
    "# generating each transaction one by one\n",
    "for i in range(num_transactions):\n",
    "    k = random.randint(min_items, max_items)   # randomly deciding how many items this customer buys\n",
    "    items = random.sample(item_pool, k)        # randomly picking k items (no duplicates)\n",
    "    transactions.append(items)                 # adding this transaction to the full list\n",
    "\n",
    "# writing the generated data into a CSV file\n",
    "with open('supermarket_transactions.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)                     # creating a CSV writer object\n",
    "    writer.writerow(['transaction_id', 'items'])   # writing the header row\n",
    "\n",
    "    # writing each transaction with its ID\n",
    "    for idx, t in enumerate(transactions):\n",
    "        writer.writerow([idx + 1, ';'.join(t)])    # items joined by \";\" inside the cell\n",
    "\n",
    "print('Saved supermarket_transactions.csv')    # confirming that the file has been saved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3201b288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved supermarket_onehot.csv\n"
     ]
    }
   ],
   "source": [
    "# Student: Hope\n",
    "\n",
    "# creating a one-hot encoded dataFrame filled with zeros (for aproiri)\n",
    "# rows = number of transactions, columns = all unique items in item_pool\n",
    "one_hot = pd.DataFrame(0, index=range(len(transactions)), columns=item_pool)\n",
    "\n",
    "# looping through every transaction and marking items as 1 where they appear\n",
    "for idx, t in enumerate(transactions):           # idx = row index, t = list of items bought\n",
    "    for item in t:                               # loop through each item in the transaction\n",
    "        one_hot.at[idx, item] = 1                # mark the item as purchased (set value to 1)\n",
    "\n",
    "# saving the one-hot encoded dataset to a CSV file\n",
    "one_hot.to_csv('supermarket_onehot.csv', index_label='transaction_id')\n",
    "\n",
    "# confirming the file has been saved successfully\n",
    "print('Saved supermarket_onehot.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0f5f13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved frequent_itemsets.csv\n"
     ]
    }
   ],
   "source": [
    "# Student: Hope\n",
    "\n",
    "# setting the minimum support threshold (5% of all transactions)\n",
    "min_support = 0.05\n",
    "\n",
    "# converting minimum support into an actual count requirement\n",
    "min_count = math.ceil(min_support * len(transactions))   # using ceil to ensure we round UP\n",
    "\n",
    "# FUNCTION: get_frequent_1_itemsets()\n",
    "# This function counts all single items and returns only those\n",
    "# whose counts satisfy the minimum support threshold.\n",
    "\n",
    "def get_frequent_1_itemsets(transactions_list):\n",
    "    counts = {}                                          # dictionary to store how many times each item appears\n",
    "    for t in transactions_list:                          # looping through each transaction\n",
    "        for item in t:                                   # looping through each item in the transaction\n",
    "            counts[item] = counts.get(item, 0) + 1       # counting occurrences of each item\n",
    "\n",
    "    # returning itemsets as frozensets so they can act as dictionary keys\n",
    "    return {\n",
    "        frozenset([item]): cnt\n",
    "        for item, cnt in counts.items()\n",
    "        if cnt >= min_count                              # keeping only items above support threshold\n",
    "    }\n",
    "\n",
    "# FUNCTION: apriori_gen()\n",
    "# Generates candidate itemsets of size k from previous frequent itemsets of size k-1.\n",
    "# Implements the Apriori joining + pruning steps.\n",
    "\n",
    "def apriori_gen(prev_freq_itemsets, k):\n",
    "    prev_itemsets = list(prev_freq_itemsets.keys())       # converting previous frequent itemsets into a list\n",
    "    candidates = set()                                    # will store all valid candidate itemsets\n",
    "\n",
    "    # join step: pair each itemset with every other itemset\n",
    "    for i in range(len(prev_itemsets)):\n",
    "        for j in range(i + 1, len(prev_itemsets)):\n",
    "            union_set = prev_itemsets[i] | prev_itemsets[j]   # union of two itemsets\n",
    "\n",
    "            if len(union_set) == k:                        # only keep unions of the correct size\n",
    "                subsets_ok = True                          # assume all subsets are valid\n",
    "\n",
    "                # pruning step: all (k-1)-subsets must be frequent\n",
    "                for subset in itertools.combinations(union_set, k - 1):\n",
    "                    if frozenset(subset) not in prev_freq_itemsets:\n",
    "                        subsets_ok = False\n",
    "                        break\n",
    "\n",
    "                if subsets_ok:\n",
    "                    candidates.add(frozenset(union_set))    # add valid candidate\n",
    "\n",
    "    return candidates                                      # return all generated candidates\n",
    "\n",
    "\n",
    "# FUNCTION: count_candidates()\n",
    "# Counts how many transactions contain each candidate itemset.\n",
    "\n",
    "def count_candidates(candidates, transactions_list):\n",
    "    counts = {c: 0 for c in candidates}                 # initialize all candidates with count = 0\n",
    "\n",
    "    for t in transactions_list:                         # go through each transaction\n",
    "        tset = set(t)                                   # convert transaction to a set for fast lookup\n",
    "        for c in candidates:                            # check each candidate\n",
    "            if c.issubset(tset):                        # if candidate is inside the transaction\n",
    "                counts[c] += 1                          # increment its support count\n",
    "\n",
    "    # return only candidates that satisfy the minimum count\n",
    "    return {c: cnt for c, cnt in counts.items() if cnt >= min_count}\n",
    "\n",
    "# Main apriori logic\n",
    "\n",
    "frequent_itemsets = {}             # dictionary to store all frequent itemsets of all sizes\n",
    "\n",
    "# first pass: find frequent 1-itemsets\n",
    "L1 = get_frequent_1_itemsets(transactions)\n",
    "frequent_itemsets.update(L1)       # add them to the global dictionary\n",
    "prev_L = L1                        # previous level's frequent itemsets\n",
    "k = 2                              # start generating 2-itemsets\n",
    "\n",
    "# iterative Apriori process\n",
    "while True:\n",
    "    candidates_k = apriori_gen(prev_L, k)               # generate candidates of size k\n",
    "\n",
    "    if not candidates_k:                                # if no candidates found → stop\n",
    "        break\n",
    "\n",
    "    Lk = count_candidates(candidates_k, transactions)    # filter candidates by support\n",
    "\n",
    "    if not Lk:                                          # if no frequent itemsets remain → stop\n",
    "        break\n",
    "\n",
    "    frequent_itemsets.update(Lk)                        # add k-itemsets to global list\n",
    "    prev_L = Lk                                         # prepare for next iteration\n",
    "    k += 1                                              # increase k (itemset size)\n",
    "\n",
    "\n",
    "# Convert results into dataframe and save\n",
    "\n",
    "\n",
    "fi_rows = []                                             # list to store rows for the CSV\n",
    "\n",
    "for itemset, cnt in frequent_itemsets.items():\n",
    "    fi_rows.append({\n",
    "        'itemset': ','.join(sorted(itemset)),           # convert set to readable string\n",
    "        'support_count': cnt,                           # raw count\n",
    "        'support': cnt / len(transactions)              # convert count → support value\n",
    "    })\n",
    "\n",
    "fi_df = pd.DataFrame(fi_rows).sort_values(\n",
    "    by='support', ascending=False\n",
    ").reset_index(drop=True)\n",
    "\n",
    "fi_df.to_csv('frequent_itemsets.csv', index=False)       # saving frequent itemsets to CSV\n",
    "\n",
    "print('Saved frequent_itemsets.csv')                     # confirmation message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffb9aefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved closed_itemsets.csv\n",
      "Saved maximal_itemsets.csv\n"
     ]
    }
   ],
   "source": [
    "# Student: Hope\n",
    "\n",
    "# Read frequent itemsets generated by Apriori\n",
    "fi_df = pd.read_csv('frequent_itemsets.csv')  # load the frequent itemsets CSV\n",
    "\n",
    "# create a mapping of itemsets (as frozensets) - support count for easy lookup\n",
    "fi_count_map = {\n",
    "    frozenset(s.split(',')): int(r['support_count'])\n",
    "    for s, r in zip(fi_df['itemset'], fi_df.to_dict('records'))\n",
    "}\n",
    "\n",
    "\n",
    "# Identify Closed Frequent Itemsets\n",
    "# A closed itemset has no superset with the same support\n",
    "\n",
    "closed_rows = []  # will store the final closed itemsets\n",
    "\n",
    "for _, row in fi_df.iterrows():                     # iterate through each frequent itemset\n",
    "    A_items = frozenset(row['itemset'].split(','))  # convert itemset string to frozenset\n",
    "    A_count = int(row['support_count'])             # get the support count of this itemset\n",
    "    is_closed = True                                # assume itemset is closed initially\n",
    "\n",
    "    # check all other itemsets for supersets with same support\n",
    "    for B_items, B_count in fi_count_map.items():\n",
    "        if A_items < B_items and A_count == B_count:  # if there exists a proper superset with same support\n",
    "            is_closed = False                          # then A is NOT closed\n",
    "            break\n",
    "\n",
    "    if is_closed:                                     # if no superset has same support\n",
    "        closed_rows.append({                           # add to closed itemsets list\n",
    "            'itemset': ','.join(sorted(A_items)),\n",
    "            'support_count': A_count,\n",
    "            'support': A_count / len(transactions)\n",
    "        })\n",
    "\n",
    "# convert to DataFrame and save to CSV\n",
    "closed_df = pd.DataFrame(closed_rows)\n",
    "closed_df.to_csv('closed_itemsets.csv', index=False)  # save closed itemsets\n",
    "print('Saved closed_itemsets.csv')                     # confirmation\n",
    "\n",
    "# Identify Maximal Frequent Itemsets\n",
    "# A maximal itemset has no frequent proper superset\n",
    "\n",
    "maximal_rows = []                                      # will store maximal itemsets\n",
    "fi_sets = [frozenset(s.split(',')) for s in fi_df['itemset']]  # list of all frequent itemsets as frozensets\n",
    "\n",
    "for A in fi_sets:                                     # iterate through each frequent itemset\n",
    "    is_maximal = True                                 # assume A is maximal initially\n",
    "    for B in fi_sets:                                 # check against all other itemsets\n",
    "        if A < B:                                     # if there exists a proper superset B\n",
    "            is_maximal = False                        # then A is NOT maximal\n",
    "            break\n",
    "\n",
    "    if is_maximal:                                    # if no superset exists\n",
    "        cnt = int(fi_df[fi_df['itemset'] == ','.join(sorted(A))]['support_count'].iloc[0])  # get support count\n",
    "        maximal_rows.append({                          # add to maximal itemsets list\n",
    "            'itemset': ','.join(sorted(A)),\n",
    "            'support_count': cnt,\n",
    "            'support': cnt / len(transactions)\n",
    "        })\n",
    "\n",
    "# convert to DataFrame and save to CSV\n",
    "maximal_df = pd.DataFrame(maximal_rows)\n",
    "maximal_df.to_csv('maximal_itemsets.csv', index=False)   # save maximal itemsets\n",
    "print('Saved maximal_itemsets.csv')                      # confirmation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
